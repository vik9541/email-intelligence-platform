# –¢–ó-005: Deploy Incident Response API [Phase 1]

**–°—Ç–∞—Ç—É—Å:** üî¥ Not Started  
**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** P1 (High - Incident Management)  
**–û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏:** 2.5h  
**–°–ª–æ–∂–Ω–æ—Å—Ç—å:** MEDIUM  
**–í–ª–∞–¥–µ–ª–µ—Ü:** Backend  
**Sprint:** Phase 1 - Production Monitoring Stack  

---

## üìã Context (–ö–æ–Ω—Ç–µ–∫—Å—Ç)

Incident Response API - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏–Ω—Ü–∏–¥–µ–Ω—Ç–∞–º–∏. –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è —Å AlertManager —á–µ—Ä–µ–∑ webhook –¥–ª—è:

- **Auto-—Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω—Ü–∏–¥–µ–Ω—Ç–æ–≤** –∏–∑ P0/P1 alerts
- **–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –ø—Ä–æ–±–ª–µ–º** (–ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–æ–≤, –º–µ—Ç—Ä–∏–∫, –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π)
- **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π** (restart, scale, cleanup)
- **–≠—Å–∫–∞–ª–∞—Ü–∏–∏ –≤ PagerDuty/Slack** –¥–ª—è critical incidents

–§–∞–π–ª `app/api/incident_response.py` —É–∂–µ —Å–æ–∑–¥–∞–Ω –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç:
- 5 REST endpoints –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏–Ω—Ü–∏–¥–µ–Ω—Ç–∞–º–∏
- Auto-remediation –ª–æ–≥–∏–∫—É
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Å Prometheus –¥–ª—è diagnostics
- PagerDuty/Slack escalation

**–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:**
- ‚úÖ –§–∞–π–ª `app/api/incident_response.py` —Å–æ–∑–¥–∞–Ω (commit 49e37eb)
- ‚úÖ Kubernetes manifest `k8s/incident-api.yaml` —Å–æ–∑–¥–∞–Ω
- ‚è∏Ô∏è **–¢—Ä–µ–±—É–µ—Ç—Å—è:** PostgreSQL database –¥–ª—è incident storage
- ‚è∏Ô∏è **–¢—Ä–µ–±—É–µ—Ç—Å—è:** Slack webhook URL, PagerDuty API key

---

## ‚úÖ Requirements (–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è)

### 1. –ú–∏–≥—Ä–∞—Ü–∏—è —Å in-memory storage –Ω–∞ PostgreSQL

–¢–µ–∫—É—â–∏–π –∫–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç `incidents: Dict[str, Incident] = {}`. –ù—É–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ PostgreSQL:

**A. –°–æ–∑–¥–∞—Ç—å Alembic migration**

```bash
# –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—É—é –º–∏–≥—Ä–∞—Ü–∏—é
alembic revision -m "add_incidents_table"
```

**B. –°–æ–∑–¥–∞—Ç—å —Ç–∞–±–ª–∏—Ü—É incidents**

```python
# alembic/versions/xxx_add_incidents_table.py

def upgrade():
    op.create_table(
        'incidents',
        sa.Column('id', sa.String(36), primary_key=True),
        sa.Column('alert_name', sa.String(100), nullable=False),
        sa.Column('priority', sa.Enum('P0', 'P1', 'P2', name='incident_priority')),
        sa.Column('status', sa.Enum('open', 'investigating', 'resolved', 'closed', name='incident_status')),
        sa.Column('created_at', sa.DateTime, default=datetime.utcnow),
        sa.Column('updated_at', sa.DateTime, default=datetime.utcnow, onupdate=datetime.utcnow),
        sa.Column('resolved_at', sa.DateTime, nullable=True),
        sa.Column('summary', sa.Text),
        sa.Column('description', sa.Text),
        sa.Column('diagnostics', sa.JSON),
        sa.Column('remediation_actions', sa.JSON),
        sa.Index('idx_incidents_status', 'status'),
        sa.Index('idx_incidents_priority', 'priority'),
        sa.Index('idx_incidents_created_at', 'created_at')
    )
```

**C. –û–±–Ω–æ–≤–∏—Ç—å –∫–æ–¥ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å PostgreSQL**

```python
# app/api/incident_response.py

from sqlalchemy import select, update
from app.database import get_db
from app.models.incident import Incident as IncidentModel

@router.post("/webhook/alert")
async def alertmanager_webhook(
    alert_payload: dict,
    db: AsyncSession = Depends(get_db)
):
    # –°–æ–∑–¥–∞—Ç—å –∏–Ω—Ü–∏–¥–µ–Ω—Ç –≤ –ë–î –≤–º–µ—Å—Ç–æ in-memory dict
    incident = IncidentModel(
        id=str(uuid.uuid4()),
        alert_name=alert_payload['labels']['alertname'],
        priority=determine_priority(alert_payload),
        status='open',
        summary=alert_payload['annotations'].get('summary', ''),
        description=alert_payload['annotations'].get('description', ''),
        diagnostics={},
        remediation_actions=[]
    )
    
    db.add(incident)
    await db.commit()
    await db.refresh(incident)
    
    # Background task –¥–ª—è diagnostics
    background_tasks.add_task(run_diagnostics, incident.id, db)
    
    return {"incident_id": incident.id}
```

### 2. –ù–∞—Å—Ç—Ä–æ–∏—Ç—å Secrets –¥–ª—è Slack –∏ PagerDuty

```bash
# Slack webhook
kubectl create secret generic slack-webhook -n production \
  --from-literal=url='https://hooks.slack.com/services/YOUR/WEBHOOK/URL'

# PagerDuty API key
kubectl create secret generic pagerduty-key -n production \
  --from-literal=api_key='YOUR_PAGERDUTY_API_KEY'

# API token –¥–ª—è authentication
kubectl create secret generic incident-api-token -n production \
  --from-literal=token='$(openssl rand -hex 32)'
```

### 3. Deploy Incident Response API

```bash
# –ü—Ä–∏–º–µ–Ω–∏—Ç—å Kubernetes manifest
kubectl apply -f k8s/incident-api.yaml -n production

# Verify: Pods running
kubectl get pods -n production -l app=incident-api

# Expected output:
# NAME                            READY   STATUS    RESTARTS   AGE
# incident-api-abc123            1/1     Running   0          30s
# incident-api-def456            1/1     Running   0          30s
```

### 4. –ù–∞—Å—Ç—Ä–æ–∏—Ç—å AlertManager webhook

–û–±–Ω–æ–≤–∏—Ç—å `prometheus/alertmanager.yml` –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ alerts –≤ Incident API:

```yaml
receivers:
  - name: 'incident-webhook'
    webhook_configs:
      - url: 'http://incident-api.production.svc.cluster.local:8080/webhook/alert'
        send_resolved: true
        http_config:
          bearer_token_file: /etc/alertmanager/secrets/incident-api-token/token
```

```bash
# –ü—Ä–∏–º–µ–Ω–∏—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
kubectl apply -f prometheus/alertmanager.yml -n monitoring

# –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å AlertManager
kubectl rollout restart deployment/alertmanager -n monitoring
```

### 5. –°–æ–∑–¥–∞—Ç—å Service –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ API

```yaml
# k8s/incident-api-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: incident-api
  namespace: production
spec:
  type: ClusterIP
  selector:
    app: incident-api
  ports:
    - port: 8080
      targetPort: 8080
      protocol: TCP
```

```bash
kubectl apply -f k8s/incident-api-service.yaml -n production
```

---

## ‚úÖ Acceptance Criteria (–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏–µ–º–∫–∏)

- [x] **AC1:** PostgreSQL migration –ø—Ä–∏–º–µ–Ω–µ–Ω–∞ –∏ —Ç–∞–±–ª–∏—Ü–∞ `incidents` —Å–æ–∑–¥–∞–Ω–∞
- [x] **AC2:** In-memory storage –∑–∞–º–µ–Ω–µ–Ω –Ω–∞ PostgreSQL –≤ –∫–æ–¥–µ
- [x] **AC3:** Secrets —Å–æ–∑–¥–∞–Ω—ã –¥–ª—è Slack, PagerDuty, API token
- [x] **AC4:** Incident API pods running (2 replicas) –≤ production namespace
- [x] **AC5:** Service `incident-api` —Å–æ–∑–¥–∞–Ω –∏ –¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ ClusterIP
- [x] **AC6:** AlertManager —É—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç webhooks –≤ Incident API
- [x] **AC7:** Test alert —Å–æ–∑–¥–∞–µ—Ç –∏–Ω—Ü–∏–¥–µ–Ω—Ç –≤ –ë–î
- [x] **AC8:** P0 incidents –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —ç—Å–∫–∞–ª–∏—Ä—É—é—Ç—Å—è –≤ PagerDuty
- [x] **AC9:** Diagnostics –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –≤ background –∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ –ë–î

---

## üß™ How to Test (–ö–∞–∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å)

### Test 1: Verify API Health

```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å health endpoint
kubectl run curl-test --rm -it --restart=Never --image=curlimages/curl -- \
  curl http://incident-api.production.svc.cluster.local:8080/health

# Expected output:
# {"status": "healthy", "database": "connected"}
```

### Test 2: Create Incident via Webhook

```bash
# –û—Ç–ø—Ä–∞–≤–∏—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–π alert —á–µ—Ä–µ–∑ AlertManager
curl -XPOST http://alertmanager.monitoring:9093/api/v1/alerts -d '[
  {
    "labels": {
      "alertname": "TestIncident",
      "severity": "critical",
      "service": "email"
    },
    "annotations": {
      "summary": "Test incident creation",
      "description": "This is a test"
    }
  }
]'

# –ü–æ–¥–æ–∂–¥–∞—Ç—å 5 —Å–µ–∫—É–Ω–¥ (webhook delay)
sleep 5

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ –∏–Ω—Ü–∏–¥–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω
kubectl run psql-test --rm -it --restart=Never --image=postgres:14 -- \
  psql -h postgres.production -U postgres -d email_db -c \
  "SELECT id, alert_name, priority, status FROM incidents WHERE alert_name='TestIncident'"

# Expected output:
#             id           | alert_name   | priority | status
# ------------------------+-------------+----------+--------
#  uuid-here              | TestIncident | P0       | open
```

### Test 3: Verify Diagnostics Run

```bash
# –°–æ–∑–¥–∞—Ç—å –∏–Ω—Ü–∏–¥–µ–Ω—Ç (—Å–º. Test 2)

# –ü–æ–¥–æ–∂–¥–∞—Ç—å 30 —Å–µ–∫—É–Ω–¥ –¥–ª—è background task
sleep 30

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ diagnostics –∑–∞–ø–æ–ª–Ω–µ–Ω—ã
curl http://incident-api.production:8080/incidents/{incident_id} | jq .diagnostics

# Expected output:
# {
#   "pods": {
#     "email-service": {
#       "ready": "3/3",
#       "status": "Running"
#     }
#   },
#   "metrics": {
#     "availability": 0.999,
#     "latency_p95": 450
#   },
#   "dependencies": {
#     "kafka": "up",
#     "postgres": "up"
#   }
# }
```

### Test 4: Verify Auto-Remediation

```bash
# –°–æ–∑–¥–∞—Ç—å –∏–Ω—Ü–∏–¥–µ–Ω—Ç —Å –≤—ã—Å–æ–∫–∏–º Kafka lag
# (Incident API –¥–æ–ª–∂–µ–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —É–≤–µ–ª–∏—á–∏—Ç—å replicas)

# –®–∞–≥ 1: –°–æ–∑–¥–∞—Ç—å Kafka lag
kubectl scale deployment email-service -n production --replicas=0
# (–≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º backlog –∫–∞–∫ –≤ –¢–ó-004)

# –®–∞–≥ 2: –°–æ–∑–¥–∞—Ç—å alert
curl -XPOST http://alertmanager.monitoring:9093/api/v1/alerts -d '[
  {
    "labels": {
      "alertname": "KafkaConsumerLagHigh",
      "severity": "warning"
    },
    "annotations": {
      "summary": "Kafka lag >15000"
    }
  }
]'

# –®–∞–≥ 3: –ü–æ–¥–æ–∂–¥–∞—Ç—å remediation
sleep 60

# –®–∞–≥ 4: –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ replicas —É–≤–µ–ª–∏—á–∏–ª–∏—Å—å
kubectl get deployment email-service -n production

# Expected: READY 6/6 (auto-scaled)

# –®–∞–≥ 5: –ü—Ä–æ–≤–µ—Ä–∏—Ç—å remediation_actions –≤ –ë–î
curl http://incident-api.production:8080/incidents/{incident_id} | jq .remediation_actions

# Expected:
# [
#   {
#     "action": "scale_deployment",
#     "target": "email-service",
#     "from": 1,
#     "to": 6,
#     "timestamp": "2025-12-14T10:30:00Z",
#     "success": true
#   }
# ]
```

### Test 5: Verify PagerDuty Escalation

```bash
# –°–æ–∑–¥–∞—Ç—å P0 incident
curl -XPOST http://alertmanager.monitoring:9093/api/v1/alerts -d '[
  {
    "labels": {
      "alertname": "EmailServiceDown",
      "severity": "critical"
    },
    "annotations": {
      "summary": "Service completely down"
    }
  }
]'

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å PagerDuty UI
# –î–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π incident:
# - Title: "EmailServiceDown - Service completely down"
# - Urgency: High
# - Assigned to: Current on-call engineer
```

### Test 6: List and Filter Incidents

```bash
# –ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –æ—Ç–∫—Ä—ã—Ç—ã–µ –∏–Ω—Ü–∏–¥–µ–Ω—Ç—ã
curl http://incident-api.production:8080/incidents?status=open | jq

# –ü–æ–ª—É—á–∏—Ç—å —Ç–æ–ª—å–∫–æ P0 –∏–Ω—Ü–∏–¥–µ–Ω—Ç—ã
curl http://incident-api.production:8080/incidents?priority=P0 | jq

# –ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ü–∏–¥–µ–Ω—Ç—ã –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞
curl "http://incident-api.production:8080/incidents?since=$(date -u -d '1 day ago' +%Y-%m-%dT%H:%M:%SZ)" | jq
```

---

## üìä Monitoring After Deployment

```bash
# Dashboard –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ Incident API
watch -n 30 '
  echo "=== Active Incidents ==="
  curl -s http://incident-api.production:8080/incidents?status=open | jq "length"
  
  echo ""
  echo "=== Incidents by Priority (last 24h) ==="
  curl -s http://prometheus.monitoring:9090/api/v1/query?query=incidents_total | jq
  
  echo ""
  echo "=== Auto-Remediation Success Rate ==="
  curl -s http://incident-api.production:8080/metrics | grep remediation_success_rate
  
  echo ""
  echo "=== API Health ==="
  kubectl get pods -n production -l app=incident-api
'
```

---

## üîß Troubleshooting

### Problem: Webhooks –Ω–µ —Å–æ–∑–¥–∞—é—Ç incidents

**Diagnosis:**
```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å logs Incident API
kubectl logs -n production -l app=incident-api --tail=50

# –ò—Å–∫–∞—Ç—å:
# - "Received webhook" ‚Üí webhook –ø—Ä–∏—à–µ–ª
# - "Error creating incident" ‚Üí –ø—Ä–æ–±–ª–µ–º–∞ —Å –ë–î
# - "Unauthorized" ‚Üí –Ω–µ–≤–µ—Ä–Ω—ã–π API token
```

**Fix:**
```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å AlertManager config
kubectl get configmap alertmanager-config -n monitoring -o yaml | grep incident-api

# –î–æ–ª–∂–µ–Ω –±—ã—Ç—å:
# - url: http://incident-api.production:8080/webhook/alert
# - bearer_token_file: /etc/alertmanager/secrets/incident-api-token/token

# –ï—Å–ª–∏ –Ω–µ–≤–µ—Ä–Ω–æ - –∏—Å–ø—Ä–∞–≤–∏—Ç—å –∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å AlertManager
```

### Problem: Diagnostics –Ω–µ –∑–∞–ø–æ–ª–Ω—è—é—Ç—Å—è

**Diagnosis:**
```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å background tasks
kubectl logs -n production -l app=incident-api | grep "Running diagnostics"

# –ï—Å–ª–∏ –Ω–µ—Ç –ª–æ–≥–æ–≤ ‚Üí background tasks –Ω–µ –∑–∞–ø—É—Å–∫–∞—é—Ç—Å—è
```

**Fix:**
```python
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ FastAPI BackgroundTasks –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ
# app/api/incident_response.py

@router.post("/webhook/alert")
async def alertmanager_webhook(
    alert_payload: dict,
    background_tasks: BackgroundTasks  # ‚Üê –í–ê–ñ–ù–û: —ç—Ç–æ –ø–∞—Ä–∞–º–µ—Ç—Ä —Ñ—É–Ω–∫—Ü–∏–∏
):
    # ...
    background_tasks.add_task(run_diagnostics, incident_id)
```

### Problem: PagerDuty incidents –Ω–µ —Å–æ–∑–¥–∞—é—Ç—Å—è

**Diagnosis:**
```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å Secret
kubectl get secret pagerduty-key -n production -o jsonpath='{.data.api_key}' | base64 -d

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å logs
kubectl logs -n production -l app=incident-api | grep pagerduty
```

**Fix:**
```bash
# Verify PagerDuty API key format
# –î–æ–ª–∂–µ–Ω –±—ã—Ç—å: –¥–ª–∏–Ω–Ω—ã–π hex string (64+ —Å–∏–º–≤–æ–ª–æ–≤)

# Test PagerDuty API manually
curl -X POST https://api.pagerduty.com/incidents \
  -H "Authorization: Token token=YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "incident": {
      "type": "incident",
      "title": "Test incident",
      "service": {
        "id": "SERVICE_ID",
        "type": "service_reference"
      }
    }
  }'
```

---

## üìã Checklist –ø–µ—Ä–µ–¥ –∑–∞–∫—Ä—ã—Ç–∏–µ–º –∑–∞–¥–∞—á–∏

- [ ] PostgreSQL migration –ø—Ä–∏–º–µ–Ω–µ–Ω–∞
- [ ] In-memory storage –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–º–µ–Ω–µ–Ω –Ω–∞ PostgreSQL
- [ ] Secrets —Å–æ–∑–¥–∞–Ω—ã –¥–ª—è Slack, PagerDuty, API token
- [ ] Incident API pods running (2 replicas)
- [ ] Service —Å–æ–∑–¥–∞–Ω –∏ –¥–æ—Å—Ç—É–ø–µ–Ω
- [ ] AlertManager webhook –Ω–∞—Å—Ç—Ä–æ–µ–Ω –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç
- [ ] Test incidents —Å–æ–∑–¥–∞—é—Ç—Å—è –≤ –ë–î
- [ ] Diagnostics –∑–∞–ø–æ–ª–Ω—è—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
- [ ] Auto-remediation —Ä–∞–±–æ—Ç–∞–µ—Ç (—Ö–æ—Ç—è –±—ã 1 —É—Å–ø–µ—à–Ω—ã–π —Å–ª—É—á–∞–π)
- [ ] PagerDuty escalation —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è P0
- [ ] Slack notifications –ø—Ä–∏—Ö–æ–¥—è—Ç
- [ ] –°–æ–∑–¥–∞–Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: "Incident Management Process" –≤ wiki

---

## üîó Related Tasks

- **Previous:** [–¢–ó-004: Implement Self-Healing Automaton](TZ-PHASE1-004-SELF-HEALING.md)
- **Next:** [–¢–ó-006: Create Monitoring Dashboard Script](TZ-PHASE1-006-MONITOR-SCRIPT.md)
- **Integration with:** –¢–ó-004 (Self-Healing –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–µ—Ç incidents —á–µ—Ä–µ–∑ —ç—Ç–æ—Ç API)

---

## üìù Notes

### Incident Lifecycle

```
Alert Fired ‚Üí Incident Created (open)
              ‚Üì
          Diagnostics Running (investigating)
              ‚Üì
          Auto-Remediation Attempted
              ‚Üì
          ‚îú‚Üí Success ‚Üí Incident Resolved (resolved)
              ‚Üì
          ‚îî‚Üí Failure ‚Üí Escalate to PagerDuty (investigating)
              ‚Üì
          Manual Resolution ‚Üí Incident Closed (closed)
```

### Auto-Remediation Logic

```python
# app/api/incident_response.py

async def attempt_remediation(incident_id: str):
    if incident.alert_name == "KafkaConsumerLagHigh":
        await scale_deployment("email-service", replicas=6)
    
    elif incident.alert_name == "PostgreSQLConnectionsHigh":
        await cleanup_postgres_connections()
    
    elif incident.alert_name == "DiskSpaceLow":
        await cleanup_old_logs()
    
    # ... –±–æ–ª–µ–µ 10 remediation scenarios
```

### Integration Points

- **AlertManager** ‚Üí Incident API (webhook)
- **Incident API** ‚Üí PagerDuty (escalation)
- **Incident API** ‚Üí Slack (notifications)
- **Incident API** ‚Üí Prometheus (diagnostics queries)
- **Incident API** ‚Üí Kubernetes (remediation actions)
- **Self-Healing Automaton** ‚Üí Incident API (create incidents for healing actions)

---

**–°–æ–∑–¥–∞–Ω–æ:** 14 –¥–µ–∫–∞–±—Ä—è 2025  
**–ê–≤—Ç–æ—Ä:** Backend Team  
**–í–µ—Ä—Å–∏—è:** 1.0
