# –¢–ó-001: Deploy Prometheus SLO Rules [Phase 1]

**–°—Ç–∞—Ç—É—Å:** üî¥ Not Started  
**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** P0 (Critical - Production Monitoring)  
**–û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏:** 1.5h  
**–°–ª–æ–∂–Ω–æ—Å—Ç—å:** MEDIUM  
**–í–ª–∞–¥–µ–ª–µ—Ü:** DevOps/SRE  
**Sprint:** Phase 1 - Production Monitoring Stack  

---

## üìã Context (–ö–æ–Ω—Ç–µ–∫—Å—Ç)

–ù—É–∂–Ω–æ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å —Å–∏—Å—Ç–µ–º—É –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ SLO –¥–ª—è production –æ–∫—Ä—É–∂–µ–Ω–∏—è. –§–∞–π–ª `prometheus/slo-rules.yaml` —É–∂–µ —Å–æ–∑–¥–∞–Ω –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç:
- 5 recording rules –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ SLI (availability, latency, error budget)
- 12 alert rules —Å multi-burn-rate –ø–æ–¥—Ö–æ–¥–æ–º (P0/P1/P2)
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å AlertManager –¥–ª—è —ç—Å–∫–∞–ª–∞—Ü–∏–∏

–ë–µ–∑ —ç—Ç–∏—Ö –ø—Ä–∞–≤–∏–ª Prometheus –Ω–µ —Å–º–æ–∂–µ—Ç –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å SLO –º–µ—Ç—Ä–∏–∫–∏ –∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∞–ª–µ—Ä—Ç—ã –ø—Ä–∏ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏ —Å–µ—Ä–≤–∏—Å–∞.

**–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:**
- ‚úÖ Prometheus —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ namespace `monitoring`
- ‚úÖ Email service –¥–µ–ø–ª–æ–π–Ω—É—Ç –≤ namespace `production`
- ‚úÖ –§–∞–π–ª `prometheus/slo-rules.yaml` —Å–æ–∑–¥–∞–Ω (commit 49e37eb)

---

## ‚úÖ Requirements (–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è)

### 1. Deploy SLO Rules –≤ Prometheus

```bash
# –ü—Ä–∏–º–µ–Ω–∏—Ç—å –ø—Ä–∞–≤–∏–ª–∞ –∫ production Prometheus
kubectl apply -f prometheus/slo-rules.yaml -n monitoring

# Verify: –ø—Ä–∞–≤–∏–ª–∞ –∑–∞–≥—Ä—É–∂–µ–Ω—ã
kubectl logs -n monitoring prometheus-0 | grep "Loading configuration file"
```

### 2. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –ø—Ä–∞–≤–∏–ª

```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–∏–Ω—Ç–∞–∫—Å–∏—Å YAML
promtool check rules prometheus/slo-rules.yaml

# Verify: –¥–æ–ª–∂–Ω–æ –≤—ã–≤–µ—Å—Ç–∏ "SUCCESS"
```

### 3. Verify recording rules —Ä–∞–±–æ—Ç–∞—é—Ç

```bash
# –ü–æ–¥–æ–∂–¥–∞—Ç—å 1 –º–∏–Ω—É—Ç—É –¥–ª—è –ø–µ—Ä–≤—ã—Ö —Ä–∞—Å—á–µ—Ç–æ–≤
sleep 60

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ recording rules —Å–æ–∑–¥–∞—é—Ç –º–µ—Ç—Ä–∏–∫–∏
curl -s http://prometheus.monitoring:9090/api/v1/query?query=slo:email_service:availability:ratio_rate5m | jq

# Expected output:
# {
#   "data": {
#     "result": [{
#       "metric": {"service": "email"},
#       "value": [timestamp, "0.999"] # –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å >0.99
#     }]
#   }
# }
```

### 4. Verify alert rules –∑–∞–≥—Ä—É–∂–µ–Ω—ã

```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–ø–∏—Å–æ–∫ –∞–∫—Ç–∏–≤–Ω—ã—Ö alert rules
curl -s http://prometheus.monitoring:9090/api/v1/rules | jq '.data.groups[] | select(.name == "slo_fast_burn") | .rules[].name'

# Expected output (12 alerts):
# - EmailServiceDown
# - SLOAvailabilityBudgetBurn
# - SLOAvailabilityCritical
# - SLOLatencyP99Critical
# - SLOErrorRateCritical
# - SLOAvailabilityWarning
# - SLOLatencyP95High
# - KafkaConsumerLagHigh
# - SLOErrorBudgetLow
# - DiskSpaceLow
# - MemoryUsageHigh
# - PodRestartingFrequently
```

### 5. Test alert triggering (dry-run)

```bash
# –í—Ä–µ–º–µ–Ω–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –Ω–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è —Ç–µ—Å—Ç–∞
kubectl patch configmap prometheus-config -n monitoring --type merge -p '
data:
  slo-rules.yaml: |
    # ... (–∏–∑–º–µ–Ω–∏—Ç—å threshold –¥–ª—è SLOAvailabilityCritical —Å 0.99 –Ω–∞ 1.01 - impossible)
'

# –ü–æ–¥–æ–∂–¥–∞—Ç—å 2 –º–∏–Ω—É—Ç—ã –∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ alert firing
curl -s http://prometheus.monitoring:9090/api/v1/alerts | jq '.data.alerts[] | select(.labels.alertname == "SLOAvailabilityCritical")'

# –û—Ç–∫–∞—Ç–∏—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏–µ
kubectl apply -f prometheus/slo-rules.yaml -n monitoring
```

---

## ‚úÖ Acceptance Criteria (–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏–µ–º–∫–∏)

- [x] **AC1:** –§–∞–π–ª `prometheus/slo-rules.yaml` –ø—Ä–∏–º–µ–Ω–µ–Ω —á–µ—Ä–µ–∑ `kubectl apply`
- [x] **AC2:** `promtool check rules` –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç SUCCESS –±–µ–∑ –æ—à–∏–±–æ–∫
- [x] **AC3:** –í—Å–µ 5 SLO recording rules –∞–∫—Ç–∏–≤–Ω—ã –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç –º–µ—Ç—Ä–∏–∫–∏:
  - `slo:email_service:availability:ratio_rate5m`
  - `slo:email_service:latency:p95`
  - `slo:email_service:latency:p99`
  - `slo:email_service:error_budget_remaining`
  - `slo:email_service:error_rate`
- [x] **AC4:** –í—Å–µ 12 alert rules –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ Prometheus
- [x] **AC5:** Recording rules –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è –∫–∞–∂–¥—ã–µ 30 —Å–µ–∫—É–Ω–¥ (–≤–∏–¥–Ω–æ –≤ Prometheus UI)
- [x] **AC6:** Alert rules –∏–º–µ—é—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ labels:
  - `severity: critical` –¥–ª—è P0
  - `severity: warning` –¥–ª—è P1
  - `severity: info` –¥–ª—è P2
- [x] **AC7:** Runbook —Å—Å—ã–ª–∫–∏ –≤ alerts –≤–µ–¥—É—Ç –Ω–∞ `docs/P0_RUNBOOK_RU.md`

---

## üß™ How to Test (–ö–∞–∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å)

### Test 1: Recording Rules Generate Metrics

```bash
# –ó–∞–ø—Ä–æ—Å–∏—Ç—å –∫–∞–∂–¥—É—é recording rule –∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ
for metric in \
  "slo:email_service:availability:ratio_rate5m" \
  "slo:email_service:latency:p95" \
  "slo:email_service:latency:p99" \
  "slo:email_service:error_budget_remaining" \
  "slo:email_service:error_rate"
do
  echo "Testing $metric..."
  result=$(curl -s "http://prometheus.monitoring:9090/api/v1/query?query=$metric" | jq -r '.data.result | length')
  
  if [ "$result" -gt 0 ]; then
    echo "‚úÖ $metric: OK ($result series)"
  else
    echo "‚ùå $metric: FAIL (no data)"
  fi
done
```

**Expected output:**
```
Testing slo:email_service:availability:ratio_rate5m...
‚úÖ slo:email_service:availability:ratio_rate5m: OK (1 series)

Testing slo:email_service:latency:p95...
‚úÖ slo:email_service:latency:p95: OK (1 series)

...
```

### Test 2: Alert Rules Evaluate Correctly

```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å evaluation time –¥–ª—è –≤—Å–µ—Ö alerts
curl -s http://prometheus.monitoring:9090/api/v1/rules | \
  jq -r '.data.groups[] | select(.name | startswith("slo_")) | .rules[] | "\(.name): \(.evaluationTime)s"'
```

**Expected output:**
```
EmailServiceDown: 0.005s
SLOAvailabilityBudgetBurn: 0.012s
SLOAvailabilityCritical: 0.008s
...
```

**Evaluation time –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å <100ms –¥–ª—è –≤—Å–µ—Ö alerts.**

### Test 3: Simulate Critical Alert

```bash
# –í—Ä–µ–º–µ–Ω–Ω–æ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å email-service
kubectl scale deployment email-service -n production --replicas=0

# –ü–æ–¥–æ–∂–¥–∞—Ç—å 2 –º–∏–Ω—É—Ç—ã (alert evaluation interval)
sleep 120

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ EmailServiceDown firing
curl -s http://prometheus.monitoring:9090/api/v1/alerts | \
  jq '.data.alerts[] | select(.labels.alertname == "EmailServiceDown" and .state == "firing")'

# Expected output:
# {
#   "labels": {
#     "alertname": "EmailServiceDown",
#     "severity": "critical",
#     "service": "email"
#   },
#   "state": "firing",
#   "annotations": {
#     "summary": "Email service has been down for 2 minutes",
#     "runbook": "docs/P0_RUNBOOK_RU.md#scenario-1-email-service-down"
#   }
# }

# Restore service
kubectl scale deployment email-service -n production --replicas=3
```

### Test 4: Verify Error Budget Calculation

```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ñ–æ—Ä–º—É–ª—É error budget
# Formula: (1 - target_slo) * time_window
# Example: (1 - 0.999) * 30 days = 0.001 * 43200 minutes = 43.2 minutes

# –ó–∞–ø—Ä–æ—Å–∏—Ç—å —Ç–µ–∫—É—â–∏–π error budget
curl -s http://prometheus.monitoring:9090/api/v1/query?query=slo:email_service:error_budget_remaining | \
  jq -r '.data.result[0].value[1]'

# Expected: —á–∏—Å–ª–æ –º–µ–∂–¥—É 0 –∏ 1 (–Ω–∞–ø—Ä–∏–º–µ—Ä, 0.85 = 85% remaining)
```

---

## üìä Monitoring After Deployment

–ü–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–≥–æ deployment –º–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å –≤ —Ç–µ—á–µ–Ω–∏–µ 24 —á–∞—Å–æ–≤:

```bash
# Dashboard –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ SLO –º–µ—Ç—Ä–∏–∫
watch -n 30 '
  echo "=== SLO Metrics ==="
  echo "Availability: $(curl -s "http://prometheus.monitoring:9090/api/v1/query?query=slo:email_service:availability:ratio_rate5m" | jq -r .data.result[0].value[1])%"
  echo "Latency P95: $(curl -s "http://prometheus.monitoring:9090/api/v1/query?query=slo:email_service:latency:p95" | jq -r .data.result[0].value[1])s"
  echo "Error Budget: $(curl -s "http://prometheus.monitoring:9090/api/v1/query?query=slo:email_service:error_budget_remaining" | jq -r .data.result[0].value[1])%"
  echo ""
  echo "=== Active Alerts ==="
  curl -s http://prometheus.monitoring:9090/api/v1/alerts | jq -r ".data.alerts[] | select(.state == \"firing\") | .labels.alertname"
'
```

---

## üîó Related Tasks

- **Next Task:** [–¢–ó-002: Deploy AlertManager Configuration](TZ-PHASE1-002-ALERTMANAGER.md)
- **Dependency:** Email service –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å deployed –≤ production
- **Blocker:** –ë–µ–∑ —ç—Ç–∏—Ö –ø—Ä–∞–≤–∏–ª AlertManager –Ω–µ –±—É–¥–µ—Ç –ø–æ–ª—É—á–∞—Ç—å alerts

---

## üìù Notes

### Prometheus Rule Groups

–§–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç 4 –≥—Ä—É–ø–ø—ã –ø—Ä–∞–≤–∏–ª:

1. **slo_recording_rules** (interval: 30s)
   - Recording rules –¥–ª—è SLI –º–µ—Ç—Ä–∏–∫
   - –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥—Ä—É–≥–∏–º–∏ alerts –∏ Grafana dashboards

2. **slo_fast_burn** (interval: 30s)
   - P0 alerts: –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ SLO –Ω–∞—Ä—É—à–µ–Ω–∏—è
   - Firing time: <2 –º–∏–Ω—É—Ç—ã

3. **slo_slow_burn** (interval: 5m)
   - P1/P2 alerts: –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –∏ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã
   - Firing time: 5-15 –º–∏–Ω—É—Ç

4. **infrastructure_alerts** (interval: 1m)
   - Infrastructure health: disk, memory, pods
   - Firing time: 1-5 –º–∏–Ω—É—Ç

### Multi-Burn-Rate Approach

–ò—Å–ø–æ–ª—å–∑—É–µ–º Google SRE best practices:
- **Fast burn (2% in 1h):** Page immediately (P0)
- **Medium burn (5% in 6h):** Alert oncall (P1)
- **Slow burn (10% in 3d):** Create ticket (P2)

–≠—Ç–æ –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ—Ç false positives –∏ —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç –∫–æ–º–∞–Ω–¥—É –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º–∞—Ö.

---

**–°–æ–∑–¥–∞–Ω–æ:** 14 –¥–µ–∫–∞–±—Ä—è 2025  
**–ê–≤—Ç–æ—Ä:** DevOps Team  
**–í–µ—Ä—Å–∏—è:** 1.0
